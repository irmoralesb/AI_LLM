{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d276fe",
   "metadata": {},
   "source": [
    "# Batching\n",
    "\n",
    "## Rate Limits\n",
    "\n",
    "* Prevents malicious attacks\n",
    "* Balance distribution of the user request\n",
    "\n",
    "\n",
    "## How rate limits occur\n",
    "\n",
    "* Too many requests\n",
    "* Too much text in the request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419bc1b",
   "metadata": {},
   "source": [
    "## Avoiding Rate Limits\n",
    "\n",
    "* Retry\n",
    "    * Short wait between requests\n",
    "* Batching\n",
    "    * Processing multiple messages in one request\n",
    "* Reducing tokens\n",
    "    * Quantifying and cutting down the number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3272e",
   "metadata": {},
   "source": [
    "## Retrying\n",
    "\n",
    "It can be configure using the tenacity library and the retry decorator\n",
    "\n",
    "The wait options are in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb6d2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "@retry(wait= wait_random_exponential(min=1, max=60),stop=stop_after_attempt(1))\n",
    "def get_response(model, messages):\n",
    "    response_value = client.chat.completions.create(\n",
    "        model= model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response_value.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e87fd",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "* Get answer in different request/responses\n",
    "* This approach is useful when the rate limit is due to the timing of the request and not the number of tokens, one way to avoid it is to send requests in batches.\n",
    "\n",
    "Here are passing the 3 countries at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477d2bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United States  \n",
      "Capital City: Washington, D.C.\n",
      "\n",
      "---\n",
      "\n",
      "Country: Ireland  \n",
      "Capital City: Dublin\n",
      "\n",
      "---\n",
      "\n",
      "Country: India  \n",
      "Capital City: New Delhi\n"
     ]
    }
   ],
   "source": [
    "countries = [\"United States\",\"Ireland\",\"India\"]\n",
    "message = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        \"content\":\"\"\"You are given a series of countries and are asked to return the country and capital city. Provide each of the questions with an answer in the response as separate content.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "[message.append({'role':'user',\"content\":f\"'{country}'\"}) for country in countries]\n",
    "\n",
    "response = get_response(\"gpt-4o-mini\",message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0cf70",
   "metadata": {},
   "source": [
    "## Reducing Tokens\n",
    "\n",
    "> Use tiktoken\n",
    "\n",
    "This helps to check for the number of tokens used before sending the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c84e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "prompt = \"Tokens can be full words, or group of characters commonly grouped together: tokenization.\"\n",
    "\n",
    "num_token=len(encoding.encode(prompt))\n",
    "print(num_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
