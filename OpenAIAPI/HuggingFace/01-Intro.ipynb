{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7b8d7e",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "It's a platform where the community can be up to date of the latest Models, Datasets and Applications.\n",
    "\n",
    "**URL**\n",
    "\n",
    "* https://huggingface.co/\n",
    "\n",
    "\n",
    "## Some Benefits\n",
    "\n",
    "* Access to Pre-trained Models for Quick Prototyping\n",
    "* End-to-End Support for ML Workflows\n",
    "* Engaged Community and Extensive Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c5aef",
   "metadata": {},
   "source": [
    "## Local Inference\n",
    "\n",
    "### Local environment\n",
    "\n",
    "* ðŸ†“ Free\n",
    "* ðŸ˜€ Convenient\n",
    "* ðŸ˜´ Slow and resource-intensive\n",
    "\n",
    "![](../resources/img/hugging-face/local-env.png)\n",
    "\n",
    "### Remote environment\n",
    "* This is through `inference providers`. \n",
    "  * https://huggingface.co/docs/inference-providers/en/index\n",
    "* ðŸ’¨Fast\n",
    "* ðŸ†“Free to get started\n",
    "\n",
    "![](../resources/img/hugging-face/inference-provider.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5b8c8",
   "metadata": {},
   "source": [
    "## Introduction to the Transformers Library\n",
    "\n",
    "* Simplifies working with **pre-trained models**\n",
    "\n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt2_pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\")\n",
    "print(gpt2_pipeline(\"What if AI\"))\n",
    "```\n",
    "```bash\n",
    "< Here comes the output >\n",
    "```\n",
    "\n",
    "**Model Card** : https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "\n",
    "### Limiting Tokens\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt2_pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\")\n",
    "results = gpt2_pipeline(\"What if AI\", max_new_tokens=10, num_return_sequences=2)\n",
    "\n",
    "for result  in results:\n",
    "    print(result['generated_text'])\n",
    "```\n",
    "Output:\n",
    "```bash\n",
    "What if AI had never existed?\n",
    "What if AI could be really smarter than us?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef7c29",
   "metadata": {},
   "source": [
    "## Using Inference Providers\n",
    "\n",
    "https://huggingface.co/docs/inference-providers/en/index\n",
    "\n",
    "```python\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=os. environ[\"HF_TOKEN\"],\n",
    ")\n",
    "```\n",
    "\n",
    "**Creating conversations**\n",
    "\n",
    "```python\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].messages)\n",
    "\n",
    "```\n",
    "\n",
    "Output\n",
    "```text\n",
    "The capital of France is **Paris**. It is known for its iconic landmarks such as\n",
    "the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
    "\n",
    "Would you like any additional information about Paris or France?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
